\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
%\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[obeyspaces]{url}

\title{Estimating an in situ light environment at MVCO}


\begin{document}
\maketitle
\section{Overview}

Our goal is to estimate the light intensity (and possible quality) at 4 m depth at MVCO to be able to relate division rates of \textit{Synechococcus} to the in situ light environment (rather than the incident light) as best we can. This is involves quite a few different measurements and quite a few different assumptions for an estimation. The pieces and how they link together are diagrammed as follows:

\section{Radiometer data processing}
\subsection{raw files to text readables}
The files that are generated from the HyperPro radiometer have a `~.raw' extension. The `~.raw` files contain all the unconverted measurements from all the sensors incorporated into the HyperPro. For this exercise though, we're really only concerned with the measurements from the MPR (depth sensor), 284 (downwelling irradiance), and 285 (solar reference). Pitch, angle, roll, upwelling irradiance, and more are also measured by the HyperPro, but for just a first glance at the ligth field at depth, these can be excluded. To convert the raw files into a readable text file, we need the calibration data for each sensor and the program SatCon, which applies the conversion from the calibration files. This can all be done in with the matlab script: \path{processPROII_KRHC.m}. The program SatCon (as long as available in the path) is called directly from within Matlab. This script does the conversion to a text file output, and then imports the textfile to make matlab files with useful raw and processed variables. The raw files can be found in : \path{\\sosiknas1\lab_data\mvco\HyperPro_Radiometer\raw_data\}. In this folder, each day's measurements and casts are contained in a folder labelled by the date. The processed .txt and .mat files are stored in a similar file structure to the one found in \url{raw_data}, where each day has it's own folder at \url{\\sosiknas1\lab_data\mvco\HyperPro_Radiometer\processed_radiometer_files}. In each day folder there are \url{converted_txtfiles} and \url{mat_outfiles} folders, the latter contains .mat files for each individual casts as well as .mat files that have data and products for all the casts for that day.

With each of the light sensors there are dark measurements - these are necessary because temperature of the sensor can affect the measurement and these are corrected with corresponding dark measurements. In the \path{processPROII_KRHC.m} script, the nearest dark measurement in time is simply subtracted from the light measurement. PAR is calculated as the integral over wavelengths 400 - 700 nm. The light measurements are then time-synced to the MPR sensor, which has more frequent measurements. Some plots for sanity-checks are also produced if the plotflag is changed to one. If it hasn't been created, the script will prompt the user for a quality comment on each cast, such as `not a cast' or `good cast'. This information is used later down the line for screening of files to use in calculation of attenuation coefficients.

The following variables in the data .mat variable for each cast are:

\begin{itemize}
\item file name
\item  cruiseID (not always entered)
\item operator  (not always entered)
\item latitude (not always entered) 
\item   longitude (not always entered)
\item    timestamp 
\item    pressure\_tare 
\item    emptyflag: data file was empty 
\item    adj\_esl: dark adjusted solar standard
\item    adj\_edl: dark adjusted downwelling
\item    esl\_PAR: solar standard PAR
\item    edl\_PAR: downwelling PAR
\item    edl\_ind: index that matches MPR data
\item    esl\_ind: index that matches MPR data
\item    mprtime: matlab date
\item    solarflag: anything fishy for how light looks?
\item    depth
\item wavelen\_solarst: wavelengths corresponding to solarstd measurements
\item wavelen\_downwell: wavelengths corresponding to downwelling measurements
\end{itemize}

\subsection{finding the lat/lon}

Unfortunately, for a lot of the casts, the position was not entered or recorded. So, this means we have to check it against the event log and sort out which recorded lat/lon goes with each cast. Not too terrible, as there will be typically be one cast per station and the timing of the cast plus depth helps to discern position. All of this is accomplished in latlon\_processing.m. It also corrects for some suspecting UTC offsets due to daylight savings time.
Below is a plot of the recovered / best guesses for lat and lon. The circles represent the known stations that were visited in past all-day long cruises at MVCO, which includes the tower and the node.

 \begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{figures_for_tex_doc/found_latlon.pdf}
\caption{Recorded latitude and longitude of each cast. Records could be from either the raw datafile or the corresponding MVCO event log. If there was a discrepancy between these two, typically the MVCO event log was favored.}
\end{figure}

\clearpage

\subsection{estimating the attenuation coefficients, k($\lambda$) and $k_{PAR}$}

Saved matlab variables (generated from \path{processPROII_KRHC.m}) are imported with the script \path{PAR_attenuation_coeffiecient_processing.m}, and used to estimate a the attenuation coefficient k for PAR. Now k can be estimated for each wavelength, but this is done in a later script (see below). K is calculated as the slope of a regression line that is fitted through log transformed data against depth of either light recorded at each wavelength or from PAR.  Smoothed depth data from the cast is used, and measurements near the top and bottom are not used. On occasion, the points had to be manually chosen to exclude bad data from a cast. The fit, values, indices and any flags are stored in a structure, K\_PAR, for each cast and save by date. This same script allows the user to load in the calculated K's and examine the fits and data points used. For some casts, a single linear regression did not seem appropriate, so the cast was split at depth (by eye) and the two pieces fit separately. A designation of `1' refers to the portion of the cast most near the surface. The various flags for the K\_PAR variable and data are:

\begin{itemize}
\item 0: good cast
\item 1: empty file or not a cast
\item 2: too short a cast to get reliable k
\item 3: split cast; expect two regressions
\end{itemize}

\noindent An aside note: it turns out that the ProSoft software made to do these types of calculations actually does not calculate an attenuation coefficient for PAR (it does so for each individual wavelength, and calculates PAR, but will NOT calculate K-PAR). 

This data is then used to generate an attenuation coefficient for each k 



\subsection{Relationships with k}

The script \path{k_relationships.m} gives some nice overview plots of where we have data for, which data we're using near the tower, and what k looks like over time. This script also imports chlorophyll data, matches k-values by date and then tries to make some relationships. Overall, things aren't terrible between k and chl, although this relationship does differ from Morel 1988 or Morel \& Maritorena 2001.

 
 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures_for_tex_doc/PAR_k_values.pdf}
\caption{Plots generated from \textbf{k\_relationships.m}. A) Position of all available and useable radiometer casts. B) K for PAR calculated. C) K PAR for just the node and tower. D). Average chlorophyll (mg/m3) over year day (chlorophyll averaged over all depths). E) Relationship between calculated K-PAR and average chlorophyll, with fitted power curves. F) Weekly climatologies of average chlorophyll and K-PAR values.}
\end{figure}

\clearpage

Another script \url{wavelength.m} looks at the color of light as it changes. For the most part, the light is indeed green, but the max wavelength reaching depths does shuffle about:
 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures_for_tex_doc/wavelengths.png}
\caption{A) and B) showing wavelength profiles at 4m just from different views. Color coded by yearday. C) Wavelength at depth with maximum intensity.}
\end{figure}


\subsection{chl-to-fluorescence match}

So, there seems to be a decent relationship between chl and k, but we only have chl values for specific dates. One way around this is to use fluorometer data as a proxy for chl and make a model from that to use in a yearday k-model


\section{Estimating a Mixed Layer Depth}

So, now that we may have a rough idea of the light level at certain times of year, we can add another layer of information to guide our estimate. And that is trying to gauge how cells might be mixing within the water column. Typically, MVCO is well mixed, being so shallow, but stratification does happen, particularly in the summer months, and we'd like to be able to say if cells are seeing all of the water column (and light levels) or just part of it (and higher or lower light levels). Density data requires temperature, salinity and depth to calculate, but we would only have some of these variables at two depths (4m at beam, 12m at node), making it difficult to estimate what the entire structure of the water column would look like. Furthermore, salinity data at some time points is unreliable, making density estimates unavailable.

One idea is to use the temperature difference between the 4m tower beam and 12m as a proxy for density difference. To see if this is a viable idea, we can examine CTD casts: look to see if they have any stratification, and how does this relate to a density difference and then temperature difference at 4 and 12 meters?  

\subsection{processing CTD casts}

The raw CTD casts are located in a folder at: \path{\\maddie\TaylorF\from_Samwise\data\MVCO\}, and in order to keep processing consistent, these raw casts are processed with matlab scripts, rather than using the Seabird software (where it might be ambiguous as to what averaging / quality control is happening). The scripts that do the processing are as follows:

\begin{itemize}
\item \path{ctd_raw2cnv_processing.m}: script that process raw CTD data (either .hex or .dat files) into readable .cnv files via Sea-Bird SBE Data (note: script should be run on a machine that has this software installed).
\item \path{import_ctd_casts.m}: imports processed CTD data (.cnv files) into a structure storage array (calls \path{import_cnv.m})
\item  \path{import_cnv.m}:  imports .cnv file into matlab variables
\end{itemize}

\noindent The .cnv (text readable) files have been temporarily stored in: \path{\\sosiknas1\Lab\data\MVCO\processed_CTD_casts\}

%'\textbackslash \textbackslash sosiknas1\textbackslash Lab\_data\textbackslash MVCO\textbackslash processed\_CTD\_casts\textbackslash`

The data at this point is still raw, and needs further quality control. This is done with the script \path{CTD_QC.m}, which imports the data structure, flags bad casts, searches for downcast portion of good casts, averages downcast data over 0.2m bins, and allows user to manually remove bad data points. Quality controlled data is stored as \path{QC_downcast.mat}. 

The next step is to identify casts as either well mixed or have some evidence of stratification, suggesting a barrier to mixing. At first, I thought I could do this using the Brunt-Vaisala frequency as an indicator, but this seems to take into account local gradients, that may or may not be indicative of true layers. (Maybe this works better for deeper water columns or coarser CTD resolution? not sure...) At any rate, Al Pludderman (from PO department) pointed me to some well-established metrics of defining a mixed layer based on changes in density or temperature from a surface reference value. In short, choose a surface reference value that you trust (CTD data within the first few meters can be unreliable), and from this value, find the depth at which density or temperature has increased or decreased past a threshold value. Some papers that look into this are Kara et al. (2000) and Brainerd and Gregg (1995), the latter which looks into how mixed layer depth metrics actually correspond to mixing.

Turns out that finding the depth at which density crosses a fixed threshold ($\Delta$) from a surface reference does a pretty good job of identifying a mixed layer. Threshold values can vary, based on season or location, but for MVCO, around $\Delta0.2$ kg/m$^3$ for density seems to do a good job. The casts taken around MVCO seem to fall into one of four types:

\begin{itemize}

\item well mixed: water column is homogeneous
\item surface level: strong stratification at the surface (typically $<$ 3 or 4 m) either from fresh water or daily warming, with layer underneath well mixed and uniform to bottom
\item mid-layers: mixed layer from surface (or under surface layer) until pycnocline appears at middle depths 
\item stratification through-out: whole water column is a pycnocline!
\end{itemize}

Most of the casts available (104 at present, but only 72 unique days) were well mixed, and from looking at the casts taken on the same day, the water column can change quite rapidly (indicating possible daily stratification and break down, which would be normal for a site at this shallow depth). 

 The  goal though is to evaluate how differences in these two measurements may serve as a proxy for temperature or density across the water column. So we can calculate the change in temperature or density from 4m and 12m in the cast and see how well this relates to the classification of the water column (which used the whole profile). In general, this is not bad (although a change in density and temperature will miss cases where there is just surface stratification). We also find that, in general, a change in temperature strongly correlates to change in density, lending support to using just temperature as a proxy for density:

 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures_for_tex_doc/dens_temp_deltas.pdf}
\caption{A) and B) shows relationship between the difference in potential density at 4 and 12 m and the difference in temperature for those two depths, recorded in the CTD cast. B) is color-coded based on stratification classification.}
\end{figure}


\noindent A $\Delta0.2$ in density corresponds to around $\sim0.65 ^{\circ}$C temperature change (based on the linear regression curve in Fig. 4B, although, one could calculate this based on equation of state of seawater...). Using this as a cutoff (or even slightly more lenient cutoff of $\sim0.6 ^{\circ}$C), we can now look over the entire timeseries of beam (4 m ) and node (12 m) temperature records to get an idea of seasonal stratification! Woo!

Looking at the daily averaged temperature records for the beam and node is not quite enough, as this average can be misleading by aggregating over day and night. Instead, we look at hourly time points to see 1) how the water column can change over the day and 2) see how many hours of the daylight portion would be considered 'stratified'. These actions are done with the script \path{examine_tempdate_hourly.m}.

It seems that the change in temperature over a day can vary quite a bit (up to a few degrees for both beam and node temperature records). The maximim difference between these two records for a given hour shows a curious seasonality, with the largest differences observed in winter and summer:

 \begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures_for_tex_doc/hourly_deltaT_by_yearday.pdf}
\caption{Hourly difference in temperature between 4 and 12 plotted against yearday for that hour.}
\end{figure}

For the second question, we find that a majority of hours during a day seems to be well mixed. With the exemption of days in summer and in winter, very few days would be stratified as to limit cell movement within the water column (although we still have no idea of how fast these cells would be mixing...).  Figure 6 illustrates this by showing a weekly bin bar graph of days available for that week and how many days had what percentage of their hours `stratified' as based on the $\Delta0.6 ^{\circ}$C proxy.

 \begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures_for_tex_doc/percentage_hours_stratified.pdf}
\caption{Bar graph showing how many days found in a given week (over the entire time series) had a certain percentage of daylight hours consideder `stratifed' based on the $\Delta0.6 ^{\circ}$C proxy. The low number at the end of the year is for leap year days.}
\end{figure}

So...in general, one can safely assume that you are dealing with a mixed water column. Now, for the light levels, we could try two values: one at 4m (assuming slow mixing) and then one with the light integrated over depth (assuming fast mixing) and see what those values look like?
%\subsection{}



\end{document}  